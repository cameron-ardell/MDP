Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 0.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 37
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 0.2
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 40
      Milliseconds taken: 8



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 0.4
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 43
      Milliseconds taken: 11



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 0.6
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 45
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 0.8
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 50
      Milliseconds taken: 10



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 50
      Milliseconds taken: 8



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 2.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 60
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 3.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 65
      Milliseconds taken: 10



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 4.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 66
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 5.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 66
      Milliseconds taken: 10



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 100.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 100
      Milliseconds taken: 12



