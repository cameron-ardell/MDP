Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.01
        Total iterations: 85
      Milliseconds taken: 11



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.03
        Total iterations: 59
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.05
        Total iterations: 47
      Milliseconds taken: 8



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.07
        Total iterations: 46
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.09
        Total iterations: 43
      Milliseconds taken: 8



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.2
        Total iterations: 32
      Milliseconds taken: 8



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.4
        Total iterations: 28
      Milliseconds taken: 8



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.6
        Total iterations: 26
      Milliseconds taken: 6



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.8
        Total iterations: 26
      Milliseconds taken: 7



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -1.0
        Total iterations: 24
      Milliseconds taken: 6



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -100.0
        Total iterations: 27
      Milliseconds taken: 7



