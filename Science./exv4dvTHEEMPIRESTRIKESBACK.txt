Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 10.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 78
      Milliseconds taken: 10



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 20.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 83
      Milliseconds taken: 11



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 30.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 84
      Milliseconds taken: 11



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 40.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 85
      Milliseconds taken: 11



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 50.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 95
      Milliseconds taken: 12



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 60.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 97
      Milliseconds taken: 12



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 70.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 98
      Milliseconds taken: 12



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 80.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 99
      Milliseconds taken: 11



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 90.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 99
      Milliseconds taken: 11



