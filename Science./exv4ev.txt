Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.0
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 52
      Milliseconds taken: 10



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.1
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 54
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.2
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 55
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.3
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 50
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.4
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 50
      Milliseconds taken: 8



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 50
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.6
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 50
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.7
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 50
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.8
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 50
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.9
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 50
      Milliseconds taken: 8



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 1.0
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 51
      Milliseconds taken: 9



