Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -0.0
               Step cost: -0.04
        Total iterations: 43
      Milliseconds taken: 11



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -0.2
               Step cost: -0.04
        Total iterations: 46
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -0.4
               Step cost: -0.04
        Total iterations: 46
      Milliseconds taken: 8



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -0.6
               Step cost: -0.04
        Total iterations: 47
      Milliseconds taken: 12



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -0.8
               Step cost: -0.04
        Total iterations: 47
      Milliseconds taken: 8



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 50
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -2.0
               Step cost: -0.04
        Total iterations: 58
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -3.0
               Step cost: -0.04
        Total iterations: 67
      Milliseconds taken: 10



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -4.0
               Step cost: -0.04
        Total iterations: 74
      Milliseconds taken: 10



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -5.0
               Step cost: -0.04
        Total iterations: 53
      Milliseconds taken: 9



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -100.0
               Step cost: -0.04
        Total iterations: 53
      Milliseconds taken: 9



