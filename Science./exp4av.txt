Policy used: value iteration
         Discount factor: 0.09
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 6
      Milliseconds taken: 3



Policy used: value iteration
         Discount factor: 0.19
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 8
      Milliseconds taken: 2



Policy used: value iteration
         Discount factor: 0.29
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 11
      Milliseconds taken: 3



Policy used: value iteration
         Discount factor: 0.39
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 14
      Milliseconds taken: 4



Policy used: value iteration
         Discount factor: 0.49
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 18
      Milliseconds taken: 4



Policy used: value iteration
         Discount factor: 0.59
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 22
      Milliseconds taken: 5



Policy used: value iteration
         Discount factor: 0.69
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 25
      Milliseconds taken: 6



Policy used: value iteration
         Discount factor: 0.79
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 30
      Milliseconds taken: 7



Policy used: value iteration
         Discount factor: 0.89
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 36
      Milliseconds taken: 8



Policy used: value iteration
         Discount factor: 0.99
     Maximum state error: 1.0E-6
    Key loss probability: 0.5
Positive terminal reward: 1.0
Negative terminal reward: -1.0
               Step cost: -0.04
        Total iterations: 50
      Milliseconds taken: 8



